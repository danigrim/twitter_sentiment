{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/daniellagrimberg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment140/training.1600000.processed.noemoticon.csv', encoding='latin-1', header = None, names=['sentiment', 'id', 'date', 'flag', 'user', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1442399</th>\n",
       "      <td>4</td>\n",
       "      <td>2061968114</td>\n",
       "      <td>Sat Jun 06 22:01:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AlexCox</td>\n",
       "      <td>Chicago with @maxdie and @derekbishe was quite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594417</th>\n",
       "      <td>4</td>\n",
       "      <td>2192143943</td>\n",
       "      <td>Tue Jun 16 06:34:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AurelieDaure</td>\n",
       "      <td>has the answer for all your IT needs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402842</th>\n",
       "      <td>0</td>\n",
       "      <td>2058034367</td>\n",
       "      <td>Sat Jun 06 14:08:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MrsNickJonas680</td>\n",
       "      <td>i almost forgot! it's d day  makes me so sadd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351269</th>\n",
       "      <td>0</td>\n",
       "      <td>2018333293</td>\n",
       "      <td>Wed Jun 03 09:48:15 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cheekyrzchick</td>\n",
       "      <td>Feel so sick.. but it aint the flu. just feel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741238</th>\n",
       "      <td>0</td>\n",
       "      <td>2266210274</td>\n",
       "      <td>Sun Jun 21 08:06:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>pindowngirl</td>\n",
       "      <td>@anna8687 awh...  too bad about the no wet t s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002699</th>\n",
       "      <td>4</td>\n",
       "      <td>1880237005</td>\n",
       "      <td>Fri May 22 00:32:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>marcieaball</td>\n",
       "      <td>Finally in the park, no one lost or in trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532008</th>\n",
       "      <td>4</td>\n",
       "      <td>2178165299</td>\n",
       "      <td>Mon Jun 15 07:17:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Lena_DISTRACTIA</td>\n",
       "      <td>@jweaving lil wayne?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524161</th>\n",
       "      <td>0</td>\n",
       "      <td>2193392702</td>\n",
       "      <td>Tue Jun 16 08:23:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>steffmd25</td>\n",
       "      <td>@pinkiecharm You're so right.  I only got a hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256225</th>\n",
       "      <td>0</td>\n",
       "      <td>1984676693</td>\n",
       "      <td>Sun May 31 15:34:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AlyYvonneG</td>\n",
       "      <td>Home now the worst part of the day is finally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382787</th>\n",
       "      <td>4</td>\n",
       "      <td>2052502650</td>\n",
       "      <td>Sat Jun 06 00:57:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PurpleLimey</td>\n",
       "      <td>@LaJacobine Hi... no, sorry I was just bored a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date      flag  \\\n",
       "1442399          4  2061968114  Sat Jun 06 22:01:25 PDT 2009  NO_QUERY   \n",
       "1594417          4  2192143943  Tue Jun 16 06:34:48 PDT 2009  NO_QUERY   \n",
       "402842           0  2058034367  Sat Jun 06 14:08:12 PDT 2009  NO_QUERY   \n",
       "351269           0  2018333293  Wed Jun 03 09:48:15 PDT 2009  NO_QUERY   \n",
       "741238           0  2266210274  Sun Jun 21 08:06:07 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1002699          4  1880237005  Fri May 22 00:32:58 PDT 2009  NO_QUERY   \n",
       "1532008          4  2178165299  Mon Jun 15 07:17:33 PDT 2009  NO_QUERY   \n",
       "524161           0  2193392702  Tue Jun 16 08:23:48 PDT 2009  NO_QUERY   \n",
       "256225           0  1984676693  Sun May 31 15:34:57 PDT 2009  NO_QUERY   \n",
       "1382787          4  2052502650  Sat Jun 06 00:57:19 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "1442399          AlexCox  Chicago with @maxdie and @derekbishe was quite...  \n",
       "1594417     AurelieDaure              has the answer for all your IT needs   \n",
       "402842   MrsNickJonas680     i almost forgot! it's d day  makes me so sadd.  \n",
       "351269     cheekyrzchick  Feel so sick.. but it aint the flu. just feel ...  \n",
       "741238       pindowngirl  @anna8687 awh...  too bad about the no wet t s...  \n",
       "...                  ...                                                ...  \n",
       "1002699      marcieaball  Finally in the park, no one lost or in trouble...  \n",
       "1532008  Lena_DISTRACTIA                              @jweaving lil wayne?   \n",
       "524161         steffmd25  @pinkiecharm You're so right.  I only got a hi...  \n",
       "256225        AlyYvonneG  Home now the worst part of the day is finally ...  \n",
       "1382787      PurpleLimey  @LaJacobine Hi... no, sorry I was just bored a...  \n",
       "\n",
       "[100000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "#using a subset of 100k tweets to lower training time\n",
    "df = df.head(100000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "y = df['sentiment']\n",
    "feats = [col for col in df.columns if col!= \"sentiment\"]\n",
    "X = df[feats]\n",
    "skf = StratifiedKFold(n_splits=5, random_state=12345, shuffle=True)\n",
    "train_index, test_index = list(skf.split(X, y))[0]\n",
    "df_test, df_train = df.iloc[test_index], df.iloc[train_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Data**\n",
    "1. Remove Duplicate Rows\n",
    "2. Remove Columns we wont use\n",
    "3. Format Target column (sentiment) into 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniellagrimberg/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop_duplicates(subset=['id'], keep='first')\n",
    "df_train = df_train.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "df_train.drop(columns=['id', 'flag', 'user'], inplace=True)\n",
    "df_test.drop(columns=['id', 'flag', 'user'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-d89fc5cdd14a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"sentiment\"] = df_test[\"sentiment\"].apply(lambda s: 1 if s!=0 else s)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"sentiment\"] = df_train[\"sentiment\"].apply(lambda s: 1 if s!=0 else s)\n",
    "df_test[\"sentiment\"] = df_test[\"sentiment\"].apply(lambda s: 1 if s!=0 else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1594417</th>\n",
       "      <td>1</td>\n",
       "      <td>Tue Jun 16 06:34:48 PDT 2009</td>\n",
       "      <td>has the answer for all your IT needs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741238</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jun 21 08:06:07 PDT 2009</td>\n",
       "      <td>@anna8687 awh...  too bad about the no wet t s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6246</th>\n",
       "      <td>0</td>\n",
       "      <td>Tue Apr 07 06:04:55 PDT 2009</td>\n",
       "      <td>@bob_lee92 great! I got my first tattoo yester...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571618</th>\n",
       "      <td>0</td>\n",
       "      <td>Wed Jun 17 09:46:55 PDT 2009</td>\n",
       "      <td>@verythat lollapalooza's has as a great a line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209292</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun May 31 23:45:50 PDT 2009</td>\n",
       "      <td>@appleseedinc Dive Shop Caroline here followin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293469</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jun 01 14:54:24 PDT 2009</td>\n",
       "      <td>At the bus stop alone  Missing dreamy guy. Oh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689010</th>\n",
       "      <td>0</td>\n",
       "      <td>Sat Jun 20 04:11:48 PDT 2009</td>\n",
       "      <td>is listening to the Solitary Snape recordings....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997561</th>\n",
       "      <td>1</td>\n",
       "      <td>Mon May 18 07:22:00 PDT 2009</td>\n",
       "      <td>@Time4CoffeeTime I am having coffee right now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002699</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri May 22 00:32:58 PDT 2009</td>\n",
       "      <td>Finally in the park, no one lost or in trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524161</th>\n",
       "      <td>0</td>\n",
       "      <td>Tue Jun 16 08:23:48 PDT 2009</td>\n",
       "      <td>@pinkiecharm You're so right.  I only got a hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                          date  \\\n",
       "1594417          1  Tue Jun 16 06:34:48 PDT 2009   \n",
       "741238           0  Sun Jun 21 08:06:07 PDT 2009   \n",
       "6246             0  Tue Apr 07 06:04:55 PDT 2009   \n",
       "571618           0  Wed Jun 17 09:46:55 PDT 2009   \n",
       "1209292          1  Sun May 31 23:45:50 PDT 2009   \n",
       "...            ...                           ...   \n",
       "293469           0  Mon Jun 01 14:54:24 PDT 2009   \n",
       "689010           0  Sat Jun 20 04:11:48 PDT 2009   \n",
       "997561           1  Mon May 18 07:22:00 PDT 2009   \n",
       "1002699          1  Fri May 22 00:32:58 PDT 2009   \n",
       "524161           0  Tue Jun 16 08:23:48 PDT 2009   \n",
       "\n",
       "                                                     tweet  \n",
       "1594417              has the answer for all your IT needs   \n",
       "741238   @anna8687 awh...  too bad about the no wet t s...  \n",
       "6246     @bob_lee92 great! I got my first tattoo yester...  \n",
       "571618   @verythat lollapalooza's has as a great a line...  \n",
       "1209292  @appleseedinc Dive Shop Caroline here followin...  \n",
       "...                                                    ...  \n",
       "293469   At the bus stop alone  Missing dreamy guy. Oh ...  \n",
       "689010   is listening to the Solitary Snape recordings....  \n",
       "997561   @Time4CoffeeTime I am having coffee right now,...  \n",
       "1002699  Finally in the park, no one lost or in trouble...  \n",
       "524161   @pinkiecharm You're so right.  I only got a hi...  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**\n",
    "\n",
    "- Note: These new columns were not used in baseline model but might be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e5f2e704e407>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"hashtags\"] = df_test[\"tweet\"].apply(lambda x: \",\".join(tag for tag in list(re.findall(r\"#(\\w+)\", x))))\n",
      "<ipython-input-10-e5f2e704e407>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"hashtag_count\"] = df_test[\"hashtags\"].apply(lambda h: len(h.split(',')) if len(h) >0 else 0)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "df_train[\"hashtags\"] = df_train[\"tweet\"].apply(lambda x: \",\".join(tag for tag in list(re.findall(r\"#(\\w+)\", x))))\n",
    "df_test[\"hashtags\"] = df_test[\"tweet\"].apply(lambda x: \",\".join(tag for tag in list(re.findall(r\"#(\\w+)\", x))))\n",
    "df_train[\"hashtag_count\"] = df_train[\"hashtags\"].apply(lambda h: len(h.split(',')) if len(h)>0 else 0)\n",
    "df_test[\"hashtag_count\"] = df_test[\"hashtags\"].apply(lambda h: len(h.split(',')) if len(h) >0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "1. Lower-case letters\n",
    "2. Remove stop words \n",
    "3. Tokenize using twitter tokenizer and lemmatize\n",
    "4. Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('@[A-Za-z0â€“9]+', ' ', text)\n",
    "    text = re.sub('#', ' ', text)\n",
    "    text = re.sub('https?:\\/\\/\\S+', ' ', text)\n",
    "    return text\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-90ff9e452201>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"tweet\"] = df_test[\"tweet\"].apply(lambda x: clean_text(x))\n"
     ]
    }
   ],
   "source": [
    "df_train[\"tweet\"] = df_train[\"tweet\"].apply(lambda x: clean_text(x))\n",
    "df_test[\"tweet\"] = df_test[\"tweet\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(s):\n",
    "    new_sent = \"\"\n",
    "    for word in s.split():\n",
    "        if word not in stop_words:\n",
    "            new_sent += \" \" + word\n",
    "    return new_sent\n",
    "\n",
    "df_train[\"tweet\"] = df_train[\"tweet\"].apply(lambda s: remove_stop_words(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-d231cf3aca09>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"tweet\"] = df_test[\"tweet\"].apply(lambda s: remove_stop_words(s))\n"
     ]
    }
   ],
   "source": [
    "df_test[\"tweet\"] = df_test[\"tweet\"].apply(lambda s: remove_stop_words(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokenize(tweet):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokenizer = TweetTokenizer()\n",
    "    new_t = []\n",
    "    for word in tokenizer.tokenize(tweet):\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "        if new_word != '':\n",
    "            new_t.append(lemmatizer.lemmatize(new_word)) \n",
    "    return \" \".join(new_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-14bab9ddb72b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"tweet\"] = df_test[\"tweet\"].apply(lambda t: lemmatize_tokenize(t))\n"
     ]
    }
   ],
   "source": [
    "df_train[\"tweet\"] = df_train[\"tweet\"].apply(lambda t: lemmatize_tokenize(t))\n",
    "df_test[\"tweet\"] = df_test[\"tweet\"].apply(lambda t: lemmatize_tokenize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling with Word2Vec & RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "documents = [text.split() for text in df_train.tweet]\n",
    "size = 200\n",
    "model = Word2Vec(size=size, window=7, min_count=10, workers=4)\n",
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9805366, 12311660)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(documents, total_examples=len(documents), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of Word2Vec model working. Notice the interesting twitter-specific voaccabulary such as h8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dislike', 0.5086536407470703),\n",
       " ('fml', 0.4550246000289917),\n",
       " ('blah', 0.4027697443962097),\n",
       " ('swear', 0.40239256620407104),\n",
       " ('boring', 0.3992295265197754),\n",
       " ('urgh', 0.398406982421875),\n",
       " ('suck', 0.39819324016571045),\n",
       " ('killing', 0.396727591753006),\n",
       " ('hating', 0.3921506106853485),\n",
       " ('h8', 0.3761572241783142)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Tokenizer to use in Flask App**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'tokenizer.pkl'\n",
    "pickle.dump(tokenizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(df_train.tweet)\n",
    "sequences_test = tokenizer.texts_to_sequences(df_test.tweet)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=100, value=0)\n",
    "\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=100, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[\"sentiment\"]\n",
    "y_test = df_test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_index = tokenizer.word_index\n",
    "\n",
    "vocab_size = len(w_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_m = np.zeros((vocab_size, size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in list(w_index.items()):\n",
    "    if word in list(model.wv.vocab.keys()):\n",
    "        embedding_m[idx] = model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, GRU,Bidirectional, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building NN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = Embedding(vocab_size, 200, weights=[embedding_m], input_length=100, trainable=False)\n",
    "nn_model.add(emb_layer)\n",
    "nn_model.add(Dropout(rate=0.4))\n",
    "nn_model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "nn_model.add(Dropout(rate=0.4))\n",
    "nn_model.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n",
    "nn_model.add(Dense(units=1, activation='sigmoid'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 200)          9616800   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 256)          336896    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 10,348,195\n",
      "Trainable params: 731,395\n",
      "Non-trainable params: 9,616,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "500/500 [==============================] - 398s 787ms/step - loss: 0.6585 - accuracy: 0.6627 - val_loss: 0.6188 - val_accuracy: 0.7292\n",
      "Epoch 2/12\n",
      "500/500 [==============================] - 384s 769ms/step - loss: 0.6158 - accuracy: 0.7190 - val_loss: 0.5829 - val_accuracy: 0.7449\n",
      "Epoch 3/12\n",
      "500/500 [==============================] - 375s 750ms/step - loss: 0.5835 - accuracy: 0.7373 - val_loss: 0.5570 - val_accuracy: 0.7534\n",
      "Epoch 4/12\n",
      "500/500 [==============================] - 384s 769ms/step - loss: 0.5590 - accuracy: 0.7475 - val_loss: 0.5408 - val_accuracy: 0.7562\n",
      "Epoch 5/12\n",
      "500/500 [==============================] - 401s 801ms/step - loss: 0.5428 - accuracy: 0.7534 - val_loss: 0.5292 - val_accuracy: 0.7602\n",
      "Epoch 6/12\n",
      "500/500 [==============================] - 432s 865ms/step - loss: 0.5270 - accuracy: 0.7587 - val_loss: 0.5191 - val_accuracy: 0.7617\n",
      "Epoch 7/12\n",
      "500/500 [==============================] - 417s 835ms/step - loss: 0.5141 - accuracy: 0.7662 - val_loss: 0.5126 - val_accuracy: 0.7595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8667f61d90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nn_model.summary()\n",
    "callbacks = [EarlyStopping(monitor='val_accuracy', patience=0)]\n",
    "nn_model.fit(X_train_seq, y_train, batch_size=128, epochs=12, validation_split=0.2, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Final Accuracy: 76%**\n",
    "\n",
    "- Saving model and weights to use in Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = nn_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "nn_model.save_weights(\"weights.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 46s 36ms/step - loss: 0.5150 - accuracy: 0.7592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5149518847465515, 0.7591999769210815]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.evaluate(X_test_seq, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Out Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction is a value from 0-1. 0 is negative, 1 is positive. We could set a threshold at for example 0.5 to determine the sentiment, but for now we are evaluating the sentiment as a continuous variable (how positive/negative)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a tweet with positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t = sequence.pad_sequences(tokenizer.texts_to_sequences([\"I love you so much but I dont like this\"]), maxlen=100, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5572156]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.predict(test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that the positive was more exagerated than negative, and in fact the tweet would be classified as positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a tweet that would be neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = sequence.pad_sequences(tokenizer.texts_to_sequences([\"I think that I will go to California next week\"]), maxlen=100, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5061608]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.predict(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a very negative tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3 = sequence.pad_sequences(tokenizer.texts_to_sequences([\"His speech was disgusting. I really don't agree with this horrible behaviour\"]), maxlen=100, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30823016]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.predict(test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a tweet we expact to be very positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_4 = sequence.pad_sequences(tokenizer.texts_to_sequences([\"The president in Colombia is the best, I would vote for him again\"]), maxlen=100, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81856334]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.predict(test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a tweet we would expect to be neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_5 = sequence.pad_sequences(tokenizer.texts_to_sequences([\"I read an article today\"]), maxlen=100, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6176232]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.predict(test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
